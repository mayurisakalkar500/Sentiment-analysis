{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parinitha-M-Samaga/DL/blob/main/Sentiment_Analysis_using_CNN_and_ML_Hybrid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "p1VpIC5MN2o8",
        "outputId": "e83b631c-acb6-4924-c4bd-96e502543bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fe909c97d046>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "hE8uZXiUMv4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n"
      ],
      "metadata": {
        "id": "b2Cg99nqzNR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OTV8kallEW17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define TensorBoard callback\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
        "\n",
        "# Define ModelCheckpoint callback to save the model\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"model_checkpoint.h5\", save_best_only=True, save_weights_only=False)\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "9mqMxi1xOPZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agvH5Ry0xr3U",
        "outputId": "a63d5ed5-65ae-4999-f87d-f0d22e7080a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "8635 1067 8635 1067\n",
            "adhi 8635\n",
            "adhi1 8635\n",
            "Epoch 1/200\n",
            "135/135 [==============================] - 11s 78ms/step - loss: 0.7033 - accuracy: 0.5266 - val_loss: 0.6777 - val_accuracy: 0.6104\n",
            "Epoch 2/200\n",
            "135/135 [==============================] - 4s 27ms/step - loss: 0.6504 - accuracy: 0.6293 - val_loss: 0.6141 - val_accuracy: 0.6938\n",
            "Epoch 3/200\n",
            "135/135 [==============================] - 3s 24ms/step - loss: 0.3906 - accuracy: 0.8471 - val_loss: 0.5915 - val_accuracy: 0.7146\n",
            "Epoch 4/200\n",
            "135/135 [==============================] - 2s 12ms/step - loss: 0.1382 - accuracy: 0.9565 - val_loss: 0.7315 - val_accuracy: 0.7156\n",
            "Epoch 5/200\n",
            "135/135 [==============================] - 1s 10ms/step - loss: 0.0658 - accuracy: 0.9812 - val_loss: 0.8556 - val_accuracy: 0.7115\n",
            "Epoch 6/200\n",
            "135/135 [==============================] - 1s 8ms/step - loss: 0.0334 - accuracy: 0.9914 - val_loss: 1.0024 - val_accuracy: 0.7135\n",
            "Epoch 7/200\n",
            "135/135 [==============================] - 1s 7ms/step - loss: 0.0234 - accuracy: 0.9951 - val_loss: 1.1465 - val_accuracy: 0.7083\n",
            "Epoch 8/200\n",
            "135/135 [==============================] - 1s 7ms/step - loss: 0.0173 - accuracy: 0.9957 - val_loss: 1.2193 - val_accuracy: 0.7198\n",
            "Epoch 9/200\n",
            "135/135 [==============================] - 1s 6ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 1.3291 - val_accuracy: 0.7083\n",
            "Epoch 10/200\n",
            "135/135 [==============================] - 1s 9ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 1.3848 - val_accuracy: 0.7177\n",
            "Epoch 11/200\n",
            "135/135 [==============================] - 1s 5ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 1.4799 - val_accuracy: 0.7156\n",
            "Epoch 12/200\n",
            "135/135 [==============================] - 1s 6ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 1.5554 - val_accuracy: 0.7063\n",
            "Epoch 13/200\n",
            "135/135 [==============================] - 1s 6ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 1.6058 - val_accuracy: 0.7083\n",
            "Epoch 13: early stopping\n",
            "270/270 [==============================] - 1s 2ms/step\n",
            "34/34 [==============================] - 0s 2ms/step\n",
            "[[0.02309966 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.17573602 0.         0.36844087 ... 0.9317682  0.         0.7355883 ]\n",
            " [0.38146302 0.         0.         ... 0.65178174 0.63112    0.10775425]\n",
            " ...\n",
            " [0.29585895 0.         0.31886873 ... 0.7009949  0.5614124  0.28978252]\n",
            " [0.3691201  0.         0.10786118 ... 0.32035515 0.85892427 0.14424217]\n",
            " [0.12673949 0.         0.         ... 0.5043355  0.34842283 0.        ]]\n",
            "Test Accuracy: 0.7160\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"\n",
        "negative_data_file = \"rt-polarity.neg\"\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 100\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.55\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text,y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
        "\n",
        "\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "print(\"adhi\",len(X_train))\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "print(\"adhi1\",len(X_train))\n",
        "# Train the Word2Vec model\n",
        "\n",
        "\n",
        "w2v_model = Word2Vec(sentences,vector_size=100, window=5, min_count=5, workers=4)\n",
        "#train=[[0 0 0 0  2 3 4 ]]\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
        "    model.add(tf.keras.layers.Conv1D(filters=num_filters, kernel_size=3, padding='valid', activation='relu'))\n",
        "    model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "    model.add(tf.keras.layers.Dropout(rate=dropout_keep_prob))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(2, activation='softmax'))  # Softmax layer for classification\n",
        "    return model\n",
        "\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Train the TextCNN model\n",
        "history = textcnn_model.fit(X_train, y_train,\n",
        "                             validation_data=(X_val, y_val),\n",
        "                             batch_size=batch_size,\n",
        "                             epochs=num_epochs, callbacks=[early_stopping_callback]\n",
        "                            )\n",
        "\n",
        "# Remove the last layers of the TextCNN model\n",
        "for layer in textcnn_model.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a new model that outputs the Flatten layer's activations\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "X_train_features = feature_extraction_model.predict(X_train)\n",
        "X_test_features = feature_extraction_model.predict(X_test)\n",
        "\n",
        "print(X_train_features)\n",
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression(penalty='l2', C=0.8)\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DObGGxmrXnHP",
        "outputId": "dc9b173c-fd52-4062-c6c0-e3b3142024dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7160\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=85, min_samples_split=5, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF_RZ2diXoN1",
        "outputId": "9891d826-abf2-44ef-faf6-8c2f381f2e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7029\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKGRlntma92P",
        "outputId": "90ed41b4-7000-4eeb-a7ed-033b0e67e909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7095\n",
            "Train Accuracy: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "# Data loading params\n",
        "dev_sample_percentage = 0.1  # Percentage of the training data to use for validation\n",
        "positive_data_file = \"rt-polarity.pos\"\n",
        "negative_data_file = \"rt-polarity.neg\"\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_filters = 128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "num_epochs = 19\n",
        "evaluate_every = 100\n",
        "checkpoint_every = 100\n",
        "num_checkpoints = 5\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "# ==================================================\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
        "positive_examples = [s.strip() for s in positive_examples]\n",
        "negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
        "negative_examples = [s.strip() for s in negative_examples]\n",
        "x_text = positive_examples + negative_examples\n",
        "y = [1 if i < len(positive_examples) else 0 for i in range(len(x_text))]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_text,y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
        "\n",
        "\n",
        "sentences = [word_tokenize(sentence) for sentence in X_train]\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_val = np.array(y_val)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_length = 100\n",
        "print(\"adhi\",len(X_train))\n",
        "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_length, padding='post')\n",
        "print(\"adhi1\",len(X_train))\n",
        "# Train the Word2Vec model\n",
        "\n",
        "\n",
        "w2v_model = Word2Vec(sentences,vector_size=100, window=5, min_count=5, workers=4)\n",
        "#train=[[0 0 0 0  2 3 4 ]]\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "def create_text_cnn():\n",
        "    # Define inputs for each channel\n",
        "    input_static = tf.keras.layers.Input(shape=(max_length,))\n",
        "    input_non_static = tf.keras.layers.Input(shape=(max_length,))\n",
        "\n",
        "    # Static channel: Use pre-trained embedding matrix and set trainable=False\n",
        "    embedding_static = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], input_length=max_length, trainable=False)(input_static)\n",
        "\n",
        "    # Non-static channel: Use separate embedding layer with trainable=True\n",
        "    embedding_non_static = tf.keras.layers.Embedding(vocab_size, embedding_matrix.shape[1], input_length=max_length, trainable=True)(input_non_static)\n",
        "\n",
        "    # Convolution and max-pooling layers for both channels with specified filter windows and 100 feature maps\n",
        "    conv_static_3 = tf.keras.layers.Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu')(embedding_static)\n",
        "    conv_static_4 = tf.keras.layers.Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu')(embedding_static)\n",
        "    conv_static_5 = tf.keras.layers.Conv1D(filters=100, kernel_size=5, padding='valid', activation='relu')(embedding_static)\n",
        "\n",
        "    conv_non_static_3 = tf.keras.layers.Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu')(embedding_non_static)\n",
        "    conv_non_static_4 = tf.keras.layers.Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu')(embedding_non_static)\n",
        "    conv_non_static_5 = tf.keras.layers.Conv1D(filters=100, kernel_size=5, padding='valid', activation='relu')(embedding_non_static)\n",
        "\n",
        "    maxpool_static_3 = tf.keras.layers.GlobalMaxPooling1D()(conv_static_3)\n",
        "    maxpool_static_4 = tf.keras.layers.GlobalMaxPooling1D()(conv_static_4)\n",
        "    maxpool_static_5 = tf.keras.layers.GlobalMaxPooling1D()(conv_static_5)\n",
        "\n",
        "    maxpool_non_static_3 = tf.keras.layers.GlobalMaxPooling1D()(conv_non_static_3)\n",
        "    maxpool_non_static_4 = tf.keras.layers.GlobalMaxPooling1D()(conv_non_static_4)\n",
        "    maxpool_non_static_5 = tf.keras.layers.GlobalMaxPooling1D()(conv_non_static_5)\n",
        "\n",
        "    # Concatenate the outputs of both channels\n",
        "    merged_static = tf.keras.layers.Concatenate()([maxpool_static_3, maxpool_static_4, maxpool_static_5])\n",
        "    merged_non_static = tf.keras.layers.Concatenate()([maxpool_non_static_3, maxpool_non_static_4, maxpool_non_static_5])\n",
        "\n",
        "    # Concatenate the outputs of both channels\n",
        "    merged = tf.keras.layers.Concatenate()([merged_static, merged_non_static])\n",
        "\n",
        "    # Dropout layer for regularization with dropout rate of 0.5\n",
        "    dropout = tf.keras.layers.Dropout(0.5)(merged)\n",
        "\n",
        "    # Output layer\n",
        "    output = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "    # Define the model with two inputs and one output\n",
        "    model = tf.keras.Model(inputs=[input_static, input_non_static], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "textcnn_model = create_text_cnn()\n",
        "textcnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Assuming X_train_static and X_train_non_static are the input data for static and non-static channels, respectively\n",
        "history = textcnn_model.fit([X_train, X_train], y_train,\n",
        "                            validation_data=([X_val, X_val], y_val),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=num_epochs,\n",
        "                            callbacks=[early_stopping_callback])\n",
        "\n",
        "\n",
        "# Remove the last layers of the TextCNN model\n",
        "for layer in textcnn_model.layers[:-3]:\n",
        "    layer.trainable = False\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define a new model that outputs the Flatten layer's activations\n",
        "feature_extraction_model = Model(inputs=textcnn_model.input, outputs=textcnn_model.layers[-2].output)\n",
        "\n",
        "# Extract features from the Flatten layer\n",
        "\n",
        "X_train_features= feature_extraction_model.predict([X_train, X_train])\n",
        "\n",
        "X_test_features = feature_extraction_model.predict([X_test,X_test])\n",
        "\n",
        "print(X_train_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XZUMGCoLT8w",
        "outputId": "fc7e961e-e876-4a42-984e-b82715b6f302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "8635 1067 8635 1067\n",
            "adhi 8635\n",
            "adhi1 8635\n",
            "Epoch 1/19\n",
            "135/135 [==============================] - 19s 118ms/step - loss: 0.6978 - accuracy: 0.5458 - val_loss: 0.6606 - val_accuracy: 0.5792\n",
            "Epoch 2/19\n",
            "135/135 [==============================] - 8s 56ms/step - loss: 0.4930 - accuracy: 0.7651 - val_loss: 0.4857 - val_accuracy: 0.7719\n",
            "Epoch 3/19\n",
            "135/135 [==============================] - 4s 30ms/step - loss: 0.2424 - accuracy: 0.9062 - val_loss: 0.5531 - val_accuracy: 0.7729\n",
            "Epoch 4/19\n",
            "135/135 [==============================] - 4s 32ms/step - loss: 0.1003 - accuracy: 0.9724 - val_loss: 0.7023 - val_accuracy: 0.7563\n",
            "Epoch 5/19\n",
            "135/135 [==============================] - 2s 13ms/step - loss: 0.0393 - accuracy: 0.9899 - val_loss: 0.8263 - val_accuracy: 0.7479\n",
            "Epoch 6/19\n",
            "135/135 [==============================] - 2s 15ms/step - loss: 0.0155 - accuracy: 0.9976 - val_loss: 0.9580 - val_accuracy: 0.7406\n",
            "Epoch 7/19\n",
            "135/135 [==============================] - 2s 13ms/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 1.0274 - val_accuracy: 0.7437\n",
            "Epoch 8/19\n",
            "135/135 [==============================] - 2s 13ms/step - loss: 0.0049 - accuracy: 0.9995 - val_loss: 1.0975 - val_accuracy: 0.7490\n",
            "Epoch 8: early stopping\n",
            "270/270 [==============================] - 1s 2ms/step\n",
            "34/34 [==============================] - 0s 8ms/step\n",
            "[[0.         0.00221465 0.         ... 0.10667519 0.         0.08194147]\n",
            " [0.         0.481316   0.02774671 ... 0.17451462 0.25240102 0.07906125]\n",
            " [0.0022419  0.26868638 0.         ... 0.46317214 0.15241331 0.625318  ]\n",
            " ...\n",
            " [0.0619553  0.20876047 0.12856184 ... 0.15334338 0.33756512 0.2327447 ]\n",
            " [0.         0.6213027  0.05650554 ... 0.2464864  0.00236471 0.26506183]\n",
            " [0.         0.12295159 0.         ... 0.26417398 0.13239585 0.39348733]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGxDWeiFzYK9",
        "outputId": "ecefb048-187a-4666-9c32-c39045f800e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8635"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model on the extracted features with L2 regularization\n",
        "logreg_model = LogisticRegression(penalty='l2', C=0.5)\n",
        "logreg_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "accuracy = logreg_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "accuracy2 = logreg_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUAn-muIW1w1",
        "outputId": "48bb306b-2af6-4243-8a40-1d926fa082ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7357\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "juKSWgHRvG97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVfDrrDvR3IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest_model = RandomForestClassifier(n_estimators=90,max_depth=5, min_samples_split=4, min_samples_leaf=2, max_features='sqrt', random_state=42)\n",
        "random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "accuracy = random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "accuracy3 = random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VCQVAxPx9mn",
        "outputId": "03f98df8-9214-4e5c-f7ae-39deffddc1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7310\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'min_samples_split': [2, 4, 6],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Instantiate the random forest classifier\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_features, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Instantiate the random forest model with the best parameters\n",
        "best_random_forest_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_random_forest_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "accuracy = best_random_forest_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "accuracy_train = best_random_forest_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "nVzuptuVmxRM",
        "outputId": "95c81b87-751f-4875-80c6-f65bbf6d06f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b17eef8f5b32>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Perform grid search to find the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Get the best parameters and best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dy-_KvjSyHbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='linear', C=0.7,gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "4ogElg7Swfol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e87e768-ef31-445d-a4ed-30dc10dbd907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7376\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model with regularization parameters\n",
        "svm_model = SVC(kernel='rbf',gamma='scale', random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "# Evaluate SVM model\n",
        "accuracy = svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_VgBFo8IL3",
        "outputId": "ddf6d680-08a2-4086-aca6-30d4e6f2f518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7357\n",
            "Train Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Instantiate the SVM classifier\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train_features, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Instantiate the SVM model with the best parameters\n",
        "best_svm_model = SVC(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "best_svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "accuracy_test = best_svm_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "\n",
        "# Evaluate the model on the training data\n",
        "accuracy_train = best_svm_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "id": "--cameTb8Ogi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize kNN classifier\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train kNN model\n",
        "knn_model.fit(X_train_features, y_train)\n",
        "\n",
        "# Evaluate kNN model\n",
        "accuracy_test = knn_model.score(X_test_features, y_test)\n",
        "print(f\"Test Accuracy: {accuracy_test:.4f}\")\n",
        "\n",
        "# Evaluate kNN model on the training data\n",
        "accuracy_train = knn_model.score(X_train_features, y_train)\n",
        "print(f\"Train Accuracy: {accuracy_train:.4f}\")\n"
      ],
      "metadata": {
        "id": "llBdlLJVr1M3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}